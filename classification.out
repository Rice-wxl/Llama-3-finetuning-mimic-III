## SLURM PROLOG ###############################################################
##    Job ID : 3864077
##  Job Name : finetune_llama3
##  Nodelist : gpu2002
##      CPUs : 
##  Mem/Node : 49152 MB
## Directory : /oscar/home/xwang259/Llama-3-finetuning-mimic-III
##   Job Started : Fri Jul 26 05:41:25 PM EDT 2024
###############################################################################
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: read).
Your token has been saved to /users/xwang259/.cache/huggingface/token
Login successful
Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:23<00:23, 11.63s/it]Downloading shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:06<00:24, 24.83s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:17<00:00, 19.53s/it]Downloading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:17<00:00, 19.30s/it]
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:23<01:11, 23.71s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:47<00:47, 23.90s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [01:12<00:24, 24.27s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:13<00:00, 14.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:13<00:00, 18.30s/it]
Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Map:   0%|          | 0/43888 [00:00<?, ? examples/s]Map:   2%|â–         | 1000/43888 [00:00<00:07, 5805.53 examples/s]Map:   5%|â–         | 2000/43888 [00:00<00:05, 7660.23 examples/s]Map:   7%|â–‹         | 3000/43888 [00:00<00:05, 8083.85 examples/s]Map:   9%|â–‰         | 4000/43888 [00:00<00:04, 8225.90 examples/s]Map:  11%|â–ˆ         | 4844/43888 [00:00<00:05, 6997.49 examples/s]Map:  14%|â–ˆâ–Ž        | 6000/43888 [00:00<00:06, 6151.00 examples/s]Map:  16%|â–ˆâ–Œ        | 7000/43888 [00:01<00:05, 6334.32 examples/s]Map:  18%|â–ˆâ–Š        | 8000/43888 [00:01<00:06, 5769.99 examples/s]Map:  21%|â–ˆâ–ˆ        | 9000/43888 [00:01<00:05, 6205.71 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 10000/43888 [00:01<00:05, 6693.45 examples/s]Map:  25%|â–ˆâ–ˆâ–Œ       | 11000/43888 [00:01<00:04, 7082.27 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 12000/43888 [00:01<00:04, 7203.03 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 13000/43888 [00:01<00:04, 7310.13 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 14000/43888 [00:02<00:05, 5838.18 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 15000/43888 [00:02<00:05, 5717.29 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 16000/43888 [00:02<00:04, 5614.81 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 17000/43888 [00:02<00:04, 6110.86 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 18000/43888 [00:02<00:03, 6497.60 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 19000/43888 [00:02<00:03, 6865.67 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20000/43888 [00:03<00:03, 6672.81 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21000/43888 [00:03<00:03, 6641.80 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22000/43888 [00:03<00:03, 6640.48 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23000/43888 [00:03<00:03, 6787.37 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24000/43888 [00:03<00:02, 6959.10 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25000/43888 [00:03<00:02, 7111.08 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26000/43888 [00:04<00:03, 5524.43 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27000/43888 [00:04<00:02, 5665.18 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28000/43888 [00:04<00:02, 5670.92 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 29000/43888 [00:04<00:02, 5925.44 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 30000/43888 [00:04<00:02, 6110.47 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31000/43888 [00:04<00:02, 6259.60 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32000/43888 [00:05<00:01, 6241.51 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33000/43888 [00:05<00:01, 6166.27 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 34000/43888 [00:05<00:01, 5462.41 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 35000/43888 [00:05<00:01, 5340.98 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36000/43888 [00:05<00:01, 5627.23 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37000/43888 [00:05<00:01, 5714.03 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38000/43888 [00:06<00:00, 6026.36 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39000/43888 [00:06<00:00, 6187.93 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 40000/43888 [00:06<00:00, 4855.44 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 41000/43888 [00:06<00:00, 5184.81 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 42000/43888 [00:06<00:00, 5484.77 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43000/43888 [00:07<00:00, 5740.56 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43888/43888 [00:07<00:00, 4524.21 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43888/43888 [00:07<00:00, 5925.04 examples/s]
Map:   0%|          | 0/43888 [00:00<?, ? examples/s]Map:   2%|â–         | 1000/43888 [00:07<05:25, 131.77 examples/s]Map:   5%|â–         | 2000/43888 [00:14<05:08, 135.89 examples/s]Map:   7%|â–‹         | 3000/43888 [00:22<05:10, 131.65 examples/s]Map:   9%|â–‰         | 4000/43888 [00:29<04:57, 134.06 examples/s]Map:  11%|â–ˆâ–        | 5000/43888 [00:37<04:50, 133.78 examples/s]Map:  14%|â–ˆâ–Ž        | 6000/43888 [00:44<04:41, 134.58 examples/s]Map:  16%|â–ˆâ–Œ        | 7000/43888 [00:52<04:35, 133.95 examples/s]Map:  18%|â–ˆâ–Š        | 8000/43888 [00:59<04:26, 134.62 examples/s]Map:  21%|â–ˆâ–ˆ        | 9000/43888 [01:07<04:20, 134.13 examples/s]Map:  23%|â–ˆâ–ˆâ–Ž       | 10000/43888 [01:14<04:10, 135.21 examples/s]Map:  25%|â–ˆâ–ˆâ–Œ       | 11000/43888 [01:22<04:05, 134.20 examples/s]Map:  27%|â–ˆâ–ˆâ–‹       | 12000/43888 [01:29<03:57, 134.22 examples/s]Map:  30%|â–ˆâ–ˆâ–‰       | 13000/43888 [01:36<03:50, 134.27 examples/s]Map:  32%|â–ˆâ–ˆâ–ˆâ–      | 14000/43888 [01:44<03:44, 133.30 examples/s]Map:  34%|â–ˆâ–ˆâ–ˆâ–      | 15000/43888 [01:52<03:36, 133.15 examples/s]Map:  36%|â–ˆâ–ˆâ–ˆâ–‹      | 16000/43888 [01:59<03:31, 131.95 examples/s]Map:  39%|â–ˆâ–ˆâ–ˆâ–Š      | 17000/43888 [02:07<03:25, 130.93 examples/s]Map:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 18000/43888 [02:15<03:20, 129.37 examples/s]Map:  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 19000/43888 [02:23<03:10, 130.47 examples/s]Map:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 20000/43888 [02:32<03:15, 122.27 examples/s]Map:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 21000/43888 [02:41<03:12, 119.08 examples/s]Map:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 22000/43888 [02:50<03:08, 116.30 examples/s]Map:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 23000/43888 [02:59<03:01, 115.26 examples/s]Map:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 24000/43888 [03:07<02:53, 114.95 examples/s]Map:  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 25000/43888 [03:16<02:45, 114.42 examples/s]Map:  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 26000/43888 [03:26<02:41, 110.49 examples/s]Map:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 27000/43888 [03:36<02:37, 107.05 examples/s]Map:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 28000/43888 [03:46<02:32, 104.26 examples/s]Map:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 29000/43888 [03:57<02:25, 102.01 examples/s]Map:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 30000/43888 [04:07<02:17, 101.13 examples/s]Map:  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 31000/43888 [04:16<02:07, 101.44 examples/s]Map:  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 32000/43888 [04:26<01:56, 102.11 examples/s]Map:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 33000/43888 [04:36<01:46, 102.43 examples/s]Map:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 34000/43888 [04:45<01:35, 103.42 examples/s]Map:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 35000/43888 [04:55<01:26, 103.15 examples/s]Map:  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 36000/43888 [05:04<01:15, 104.19 examples/s]Map:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 37000/43888 [05:14<01:06, 103.62 examples/s]Map:  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 38000/43888 [05:24<00:56, 103.50 examples/s]Map:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 39000/43888 [05:34<00:47, 102.93 examples/s]Map:  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 40000/43888 [05:43<00:37, 103.42 examples/s]Map:  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 41000/43888 [05:53<00:27, 104.30 examples/s]Map:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 42000/43888 [06:02<00:18, 104.45 examples/s]Map:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 43000/43888 [06:12<00:08, 104.67 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43888/43888 [06:20<00:00, 104.17 examples/s]Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 43888/43888 [06:21<00:00, 115.14 examples/s]
ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.
Traceback (most recent call last):
  File "/oscar/home/xwang259/Llama-3-finetuning-mimic-III/classification.py", line 80, in <module>
    trainer = Trainer(
              ^^^^^^^^
  File "/users/xwang259/.conda/envs/llm_finetune/lib/python3.11/site-packages/transformers/trainer.py", line 529, in __init__
    self._move_model_to_device(model, args.device)
  File "/users/xwang259/.conda/envs/llm_finetune/lib/python3.11/site-packages/transformers/trainer.py", line 776, in _move_model_to_device
    model = model.to(device)
            ^^^^^^^^^^^^^^^^
  File "/users/xwang259/.conda/envs/llm_finetune/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1160, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/users/xwang259/.conda/envs/llm_finetune/lib/python3.11/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/xwang259/.conda/envs/llm_finetune/lib/python3.11/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  File "/users/xwang259/.conda/envs/llm_finetune/lib/python3.11/site-packages/torch/nn/modules/module.py", line 810, in _apply
    module._apply(fn)
  [Previous line repeated 5 more times]
  File "/users/xwang259/.conda/envs/llm_finetune/lib/python3.11/site-packages/torch/nn/modules/module.py", line 833, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/users/xwang259/.conda/envs/llm_finetune/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1158, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 23.64 GiB of which 91.25 MiB is free. Including non-PyTorch memory, this process has 23.53 GiB memory in use. Of the allocated memory 23.37 GiB is allocated by PyTorch, and 1.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
